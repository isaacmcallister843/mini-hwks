{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "21bcf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from collections import deque\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sys.path.append(\"hawkeslib-master/\")\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from hawkeslib import MultivariateExpHawkesProcess as MVH\n",
    "\n",
    "\n",
    "if not hasattr(np, \"float\"):\n",
    "    np.float = float\n",
    "if not hasattr(np, \"int\"):\n",
    "    np.int = int\n",
    "if not hasattr(np, \"bool\"):\n",
    "    np.bool = bool\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cddab4",
   "metadata": {},
   "source": [
    "# Methods and Classes\n",
    "Eventaully be moved into a package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7085aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hawkes_system(): \n",
    "\n",
    "    def __init__(self, \n",
    "                 input_buffer_window_short = 10, input_buffer_window_long = 30, \n",
    "                 residual_window=15, \n",
    "                 OFI_buffer_window_short = 10, OFI_buffer_window_long = 25, \n",
    "                 theta = .1, hawkes_only_features = False, active_states =  ['MU', 'MD', \"ADNM\"]): \n",
    "        \n",
    "        # Buffers are used for direct computations \n",
    "        # hist vectors are used for plotting later \n",
    "\n",
    "        # ----------- Hawkes Paramaeters \n",
    "        # Controls the functionality of only computing the hawkes intensities \n",
    "        self.hawkes_only_features  = hawkes_only_features\n",
    "\n",
    "        # Setup default parameters, learned from previous bet fit experiments \n",
    "        self.mu = np.array([0.22742546, 0.23532417, 0.01620813])\n",
    "        self.a = np.array([\n",
    "            [0.19858365, 0.107086  , 0.00931048],\n",
    "            [0.12042455, 0.17995655, 0.01154943],\n",
    "            [0.08805226, 0.06513124, 0.17139197]\n",
    "            ])\n",
    "        \n",
    "        self.theta= theta\n",
    "\n",
    "        # Useful for simulating the proccess \n",
    "        self.hawkes_gamma_model = MVH()\n",
    "        self.hawkes_gamma_model.set_params(mu = self.mu, A = self.a, theta = self.theta)\n",
    "\n",
    "        # Record active states \n",
    "        self.active_states = active_states\n",
    "        self.state_map = {j : i for i,j in enumerate(self.active_states )}\n",
    "        self.num_states = len(self.active_states)\n",
    "\n",
    "        # ----------- Basic Data handling\n",
    "        self.i = 0 # Counts the number of states that have been successfully read (non idle)\n",
    "\n",
    "        # Define two buffers \n",
    "        # short buffer: used for short features\n",
    "        # long buffer: used for long features \n",
    "        self.input_buffer_window_short = input_buffer_window_short\n",
    "        self.input_buffer_window_long  = input_buffer_window_long\n",
    "\n",
    "        self.input_buffer_short = deque(maxlen=input_buffer_window_short) \n",
    "        self.input_buffer_long  = deque(maxlen=input_buffer_window_long) \n",
    "        self.input_hist = deque()\n",
    "    \n",
    "        # Reference indexs - not great\n",
    "        self.time_idx =0 \n",
    "        self.state_idx = 1\n",
    "        self.price_off_open = 2\n",
    "        self.bid_px_idx = 4\n",
    "        self.ask_px_idx = 5\n",
    "        self.bid_sz_idx = 6\n",
    "        self.ask_sz_idx = 7\n",
    "        self.mid_price_idx = 8\n",
    "\n",
    "        # ----------- Hawkes variables and state managment \n",
    "        # Define hawkes inputs, just derived from the current buffers \n",
    "        self.t_c = deque() \n",
    "        self.t_start = None # actual time value to start from \n",
    "\n",
    "        # Dealing with states \n",
    "        self.current_state = np.zeros((self.num_states, self.num_states))\n",
    "        self.state_up_to_date = False #boolean to determine if the state has been updated yet \n",
    "        self.state_hist = deque()\n",
    "\n",
    "        # ---------- Residual and compensator managment\n",
    "        # Compensator \n",
    "        self.dLambda_hist = deque() \n",
    "        self.dLambda_buffer = deque(maxlen=3) \n",
    "\n",
    "        # Residual and delta setup \n",
    "        self.residual_window = residual_window\n",
    "\n",
    "        # used directly for computing compensator values \n",
    "        self.residual_buffer = [deque(maxlen=residual_window) for i in range(self.num_states)]\n",
    "        self.residual_hist = [deque() for i in range(self.num_states)] # stores residual and time values\n",
    "        [element.append(0.0) for element in self.residual_buffer]\n",
    "\n",
    "        # Just used for storing the delta history \n",
    "        self.delta_hist = [deque() for i in range(self.num_states)]\n",
    "        self.delta_buffer = [deque(maxlen=2) for i in range(self.num_states)]\n",
    "        self.one_step_res = [deque() for i in range(self.num_states)]\n",
    "\n",
    "        # ---------- Output feature managment \n",
    "        # Price features\n",
    "        self.open_price = None \n",
    "\n",
    "        # Feature history \n",
    "        self.feature_hist = deque()\n",
    "\n",
    "        # Order flow imbalance\n",
    "        self.OFI_buffer_window_short  = OFI_buffer_window_short\n",
    "        self.OFI_buffer_window_long = OFI_buffer_window_long\n",
    "        self.OFI_buffer_short = deque(maxlen = OFI_buffer_window_short)\n",
    "        self.OFI_buffer_long = deque(maxlen  = OFI_buffer_window_long)\n",
    "\n",
    "\n",
    "    def read_pd_raw(self, row_curr : pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Reads in rows from BBO -1s order book information, parses and updates all buffers and variables \n",
    "        \"\"\"\n",
    "        # Check if row is valid\n",
    "        valid_ind =  row_curr[['ts_event', 'price', 'bid_px_00', 'ask_px_00', 'ts_recv', 'bid_sz_00', 'ask_sz_00' ]].isna().sum(axis=1)\n",
    "        if (valid_ind != 0).any(): \n",
    "            return \n",
    "\n",
    "        # If row is valid compute mid prie \n",
    "        row_curr['mid_price'] = (row_curr['bid_px_00'] + row_curr['ask_px_00']) / 2\n",
    "        row_curr_temp =  row_curr[['ts_recv', 'price', 'bid_px_00', \n",
    "                                   'ask_px_00', 'bid_sz_00', 'ask_sz_00', \n",
    "                                   'mid_price']].to_numpy()[0]\n",
    "        \n",
    "        # Final format for row_curr_arr (time_sec, state, price_off_open, price, bid_px, ask_px, bid_sz, ask_sz, mid_price) \n",
    "\n",
    "        # Allocate 0s want to include space for the \n",
    "        row_curr_arr = np.zeros(len(row_curr_temp)+2, dtype = float) # size 9 \n",
    "\n",
    "        # Fill with numeric values \n",
    "        row_curr_arr[3:] = row_curr_temp[1:] # 3-10 filled with values\n",
    "\n",
    "        # Convert to time since midnight \n",
    "        dt_seconds = np.datetime64(row_curr_temp[0], 's')       \n",
    "\n",
    "        # Add second values, 0 filled \n",
    "        row_curr_arr[self.time_idx] = (dt_seconds.astype('datetime64[s]') - dt_seconds.astype('datetime64[D]'))/ np.timedelta64(1, 's')\n",
    "\n",
    "        if self.i == 0: \n",
    "            self.open_price = row_curr_arr[self.mid_price_idx]\n",
    "            row_curr_arr[self.state_idx] = -10\n",
    "            row_curr_arr[2] = row_curr_arr[self.mid_price_idx] - self.open_price\n",
    "\n",
    "            self.input_buffer_short.append(row_curr_arr)\n",
    "            self.input_buffer_long.append(row_curr_arr)\n",
    "            self.i += 1 \n",
    "            return \n",
    "        \n",
    "\n",
    "        # Add the price off open   \n",
    "        row_curr_arr[2] = row_curr_arr[self.mid_price_idx] - self.open_price\n",
    "        row_prev_arr = self.input_buffer_short[-1]\n",
    "\n",
    "        # Compute states based on short buffer value \n",
    "        if row_prev_arr[self.mid_price_idx]  < row_curr_arr[self.mid_price_idx]: \n",
    "            # mid price increases \n",
    "            row_curr_arr[self.state_idx] = 0 # MU \n",
    "        elif row_curr_arr[self.mid_price_idx] < row_prev_arr[self.mid_price_idx]: \n",
    "            # mid price down \n",
    "            row_curr_arr[self.state_idx] = 1 # MD \n",
    "        elif ((row_curr_arr[self.mid_price_idx] == row_prev_arr[self.mid_price_idx]) \n",
    "              & (row_curr_arr[self.ask_sz_idx] < row_prev_arr[self.ask_sz_idx])): \n",
    "            # if the mid price stays the same,  and the current ask size has decreased, then Ask Down No Movement \n",
    "            row_curr_arr[self.state_idx] = 2 # ADNM \n",
    "        else: # set to idle state \n",
    "            row_curr_arr[self.state_idx] = -10 # IDLE \n",
    "\n",
    "        # Set start date if needed\n",
    "        if self.t_start is None and row_curr_arr[self.state_idx] != -10: \n",
    "            # First non idle time \n",
    "            self.t_start = row_curr_arr[self.time_idx]\n",
    "        elif row_curr_arr[self.state_idx] != -10: \n",
    "            self.main_update(row_curr_arr)\n",
    "\n",
    "        # Update long buffer \n",
    "        self.input_buffer_short.append(row_curr_arr)\n",
    "        self.input_buffer_long.append(row_curr_arr)\n",
    "        self.input_hist.append(row_curr_arr)\n",
    "        \n",
    "        # Update i counter \n",
    "        self.i += 1\n",
    "\n",
    "    def main_update(self, row_curr_arr): \n",
    "        '''\n",
    "        Responsible for updating hawkes states, compensators, deltas, and buffers \n",
    "        '''\n",
    "        # if t_start already set and state isnt idle\n",
    "        # add the time since t_start and the state  - this is the result that will be use for the hawkes proccess directly \n",
    "        t_ = row_curr_arr[self.time_idx] - self.t_start \n",
    "        c_ = row_curr_arr[self.state_idx]\n",
    "        self.t_c.append(np.array([t_, c_ ]))\n",
    "\n",
    "        # Tell the rest of the program the state is now out of date \n",
    "        self.state_up_to_date = False \n",
    "\n",
    "        # Update the states\n",
    "        self.update_states()\n",
    "\n",
    "        # Update compensator increments \n",
    "        self.update_compensator_inc()\n",
    "\n",
    "        # Update the residuals \n",
    "        self.update_residuals_delta()\n",
    "\n",
    "    def update_states(self):\n",
    "        # just a functin of t and c and the parameters \n",
    "        if self.state_up_to_date: \n",
    "            return  \"\"\n",
    "        if len(self.t_c) < 2: \n",
    "            return \"\"\n",
    "        \n",
    "        t_cur, c_cur = self.t_c[-1]\n",
    "        t_prev, c_prev = self.t_c[-2]\n",
    "\n",
    "        c_cur = int(c_cur)\n",
    "\n",
    "        dt = t_cur - t_prev\n",
    "        if dt < 0 : \n",
    "            print('non montone')\n",
    "        else:\n",
    "            # Compute new state and update\n",
    "            int_state_decay = np.exp(-self.theta * dt) \n",
    "            self.current_state = self.current_state * int_state_decay\n",
    "            self.current_state[:, c_cur] = self.current_state[:, c_cur] +  1 \n",
    "            \n",
    "            # add to state history \n",
    "            self.state_hist.append(self.current_state.copy())\n",
    "            self.state_up_to_date = True \n",
    "\n",
    "            #print(\"Update state successful\")\n",
    "\n",
    "            # This stores intensities AFTER the update \n",
    "\n",
    "    def update_features(self):\n",
    "        \"\"\"\n",
    "        Outputs variables at current state \n",
    "        1. Hawkes intesities for each event\n",
    "        2. Compensated intensties \n",
    "        3. Mid price movement off start \n",
    "        4. Derivative for mid price (with smoothing applied )\n",
    "        5. Order-flow imbalance (signed volume) \n",
    "        6. Queue (book) imbalance (top of book)\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.t_c) <= 2:\n",
    "            return  \n",
    "    \n",
    "        if len(self.input_buffer_short) < 2:\n",
    "            return\n",
    "\n",
    "        t_cur, c_cur = self.t_c[-1]\n",
    "        \n",
    "        # -------- Intensities  \n",
    "        lam = np.sum(self.current_state * self.theta  * self.a, axis = 1)  + self.mu\n",
    "        lam_compensated = np.zeros(self.num_states)\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            if len(self.delta_buffer[i])  >= 1:\n",
    "                lam_compensated[i] =  lam[i] *self.delta_buffer[i][-1]\n",
    "        \n",
    "        intensity_spread = lam[0] - lam[1]\n",
    "        intensity_spread_compensated = lam_compensated[0] - lam_compensated[1]\n",
    "\n",
    "        if not self.hawkes_only_features: \n",
    "            # -------- Price off movement \n",
    "            price_off_open = self.input_buffer_short[-1][self.price_off_open]\n",
    "\n",
    "            # --------  Mid price derivative + short term volatility \n",
    "            price_mov_derivative_s = np.nan  \n",
    "            vol_s = np.nan \n",
    "            momentum_s = np.nan \n",
    "\n",
    "            if len(self.input_buffer_short ) >= self.input_buffer_window_short: \n",
    "                mid_prices_s = np.array(self.input_buffer_short)[:, self.mid_price_idx]\n",
    "                \n",
    "                # Price derivates \n",
    "                x_s = np.arange(self.input_buffer_window_short)\n",
    "                price_mov_derivative_s, intercept = np.polyfit(x_s, mid_prices_s, 1)\n",
    "\n",
    "                # Short term volatility \n",
    "                vol_s = np.std(np.diff(mid_prices_s))\n",
    "\n",
    "                # momentum \n",
    "                window_m_s = min(5, self.input_buffer_window_short - 1)\n",
    "                if window_m_s > 0: momentum_s = mid_prices_s[-1] - mid_prices_s[-1 - window_m_s]\n",
    "\n",
    "\n",
    "            price_mov_derivative_l = np.nan  \n",
    "            vol_l = np.nan \n",
    "            momentum_l = np.nan \n",
    "\n",
    "            if len(self.input_buffer_long ) >= self.input_buffer_window_long: \n",
    "                mid_prices_l = np.array(self.input_buffer_long)[:, self.mid_price_idx]\n",
    "                \n",
    "                # Price derivates \n",
    "                x_l = np.arange(self.input_buffer_window_long)\n",
    "                price_mov_derivative_l, intercept = np.polyfit(x_l, mid_prices_l, 1)\n",
    "\n",
    "                # Short term volatility \n",
    "                vol_l = np.std(np.diff(mid_prices_l))\n",
    "\n",
    "                # momentum \n",
    "                momentum_l = mid_prices_l[-1] - mid_prices_l[0]\n",
    "\n",
    "            # -------- Order flow imablance (OFI)\n",
    "            OFI_inc = 0 \n",
    "            current_arr = self.input_buffer_short[-1]\n",
    "            prev_arr = self.input_buffer_short[-2] \n",
    "\n",
    "            if current_arr[self.bid_px_idx] > prev_arr[self.bid_px_idx]: OFI_inc += current_arr[self.bid_sz_idx]\n",
    "            elif current_arr[self.bid_px_idx] < prev_arr[self.bid_px_idx]: OFI_inc  -= prev_arr[self.bid_sz_idx]\n",
    "\n",
    "            if current_arr[self.ask_px_idx] < prev_arr[self.ask_px_idx]: OFI_inc -= prev_arr[self.ask_sz_idx]\n",
    "            elif current_arr[self.ask_px_idx] > prev_arr[self.ask_px_idx]: OFI_inc += current_arr[self.ask_sz_idx] \n",
    "            \n",
    "            # ---- store into BOTH buffers\n",
    "            self.OFI_buffer_short.append(OFI_inc)\n",
    "            self.OFI_buffer_long.append(OFI_inc)\n",
    "\n",
    "            # ---- compute short cumulative\n",
    "            OFI_cum_short = np.nan\n",
    "            if len(self.OFI_buffer_short) >= self.OFI_buffer_window_short:\n",
    "                OFI_cum_short = float(np.sum(self.OFI_buffer_short))\n",
    "\n",
    "            # ---- compute long cumulative\n",
    "            OFI_cum_long = np.nan\n",
    "            if len(self.OFI_buffer_long) >= self.OFI_buffer_window_long:\n",
    "                OFI_cum_long = float(np.sum(self.OFI_buffer_long))\n",
    "\n",
    "\n",
    "            # -------- Queue imbalance (QI)\n",
    "            bid_sz = current_arr[self.bid_sz_idx]\n",
    "            ask_sz = current_arr[self.ask_sz_idx]\n",
    "            QI = bid_sz / (bid_sz + ask_sz) if bid_sz + ask_sz > 0 else .5 \n",
    "\n",
    "            # LQ ratio\n",
    "            liq_ratio = bid_sz / max(ask_sz, 1e-9)\n",
    "            liq_ratio = np.clip(liq_ratio,0, 100)\n",
    "\n",
    "            # -------- Bid spread \n",
    "            spread = current_arr[self.ask_px_idx] - current_arr[self.bid_px_idx]\n",
    "\n",
    "            # ------- Create and export feature stack \n",
    "            # (lam, lam_compensated, price_mov_derivative, OFI, QI, mid_price, price_off_open, t_cur )\n",
    "            features = np.concatenate((lam, lam_compensated, \n",
    "                                    np.array(\n",
    "                                        [intensity_spread, intensity_spread_compensated, \n",
    "                                         spread, liq_ratio,\n",
    "                                         OFI_inc, OFI_cum_short, OFI_cum_long, \n",
    "                                         QI, current_arr[self.mid_price_idx], price_off_open,\n",
    "                                         price_mov_derivative_s, vol_s, momentum_s,\n",
    "                                         price_mov_derivative_l, vol_l, momentum_l,\n",
    "                                         t_cur]\n",
    "                                    )))\n",
    "        else: \n",
    "            features = np.concatenate((lam, lam_compensated))\n",
    "\n",
    "        self.feature_hist.append(features)\n",
    "\n",
    "    def update_compensator_inc(self): \n",
    "        \"\"\"\n",
    "        This gives us the per interval delta Lambda \n",
    "        Or how much area under the intensity curve is added between events \n",
    "        \"\"\"\n",
    "        if len(self.state_hist) < 2: \n",
    "            return \n",
    "\n",
    "        # Only update if states are up to date \n",
    "        if self.state_up_to_date: \n",
    "            # Get state variable \n",
    "            s_plus_prev = self.state_hist[-2]\n",
    "\n",
    "            # Get recent t values and one previous \n",
    "            t_cur, c_cur = self.t_c[-1]\n",
    "            t_prev, c_prev = self.t_c[-2]\n",
    "\n",
    "            dt = t_cur - t_prev\n",
    "\n",
    "            if dt <= 0:\n",
    "                print(\"non monotone\")\n",
    "                # Non-monotone times → better to bail than accumulate nonsense\n",
    "                return \n",
    "            \n",
    "\n",
    "            decay_factor = 1.0 - np.exp(-self.theta * dt )\n",
    "            \n",
    "            base_term = self.mu * dt                \n",
    "            excite_term = (self.a * s_plus_prev).sum(axis=1)  * decay_factor\n",
    "\n",
    "            dLambda = np.zeros((1,  self.num_states+ 1 ))[0]\n",
    "\n",
    "            dLambda[0:self.num_states] = base_term + excite_term   \n",
    "            dLambda[-1] = c_cur\n",
    "            \n",
    "            # Store in history (vector per interval)\n",
    "            self.dLambda_hist.append(np.concatenate((dLambda, np.array([t_cur]))))\n",
    "            self.dLambda_buffer.append(dLambda)\n",
    "\n",
    "    def update_residuals_delta(self):\n",
    "        # 1. guard\n",
    "        if len(self.dLambda_buffer) < 2 :\n",
    "            return\n",
    "\n",
    "        # Get most recent dLambd value \n",
    "        dLambda = self.dLambda_buffer[-1]\n",
    "\n",
    "        # Get most recent c value \n",
    "        t_cur, c_cur = self.t_c[-1]\n",
    "        k = int(c_cur)\n",
    "\n",
    "        # Update residuals for all states \n",
    "        for j in range(self.num_states):\n",
    "            new_res = self.residual_buffer[j][-1] + dLambda[j]\n",
    "            self.residual_buffer[j][-1] = new_res\n",
    "\n",
    "        # Grab the resiudal for the state that just fired \n",
    "        state_k_residuals = np.array(self.residual_buffer[k])\n",
    "\n",
    "        # Grab the most recent residual \n",
    "        raw_res = state_k_residuals[-1]\n",
    "        \n",
    "        # Want to now compute the most recent delta, and the one step error \n",
    "\n",
    "        # First check if we have a full window yet \n",
    "        if len(state_k_residuals) >= self.residual_window:\n",
    "\n",
    "            # Compute the delta of the most recent window \n",
    "            mean_res = state_k_residuals.mean()\n",
    "            if mean_res <=0 : \n",
    "                return \n",
    "            delta_k = 1.0 / state_k_residuals.mean()\n",
    "\n",
    "            # Add it to the history \n",
    "            self.delta_hist[k].append(np.array([delta_k, t_cur]))\n",
    "\n",
    "            self.delta_buffer[k].append(delta_k)\n",
    "\n",
    "            # If we have enough deltas compute the one step resiudal \n",
    "            if len(self.delta_buffer[k]) > 1:\n",
    "\n",
    "                # Grab previously computed delta \n",
    "                prev_delta = self.delta_buffer[k][-2]\n",
    "\n",
    "                # Compensate current residual with previous delta \n",
    "                comp_res = raw_res * prev_delta\n",
    "\n",
    "                # Get the one step error and append \n",
    "                self.one_step_res[k].append(np.array([comp_res, t_cur]))\n",
    "\n",
    "\n",
    "        # For the state that just fired, resent the cummulative count \n",
    "        self.residual_hist[k].append(np.array([raw_res, t_cur]))\n",
    "        self.residual_buffer[k].append(0.0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e745717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_frames(hwk: hawkes_system): \n",
    "    \n",
    "    feature_hist_arr = np.array(hwk.feature_hist)\n",
    "\n",
    "    feature_names = [\n",
    "        \"lam_0\", \"lam_1\", \"lam_2\",\n",
    "        \"lam_comp_0\", \"lam_comp_1\", \"lam_comp_2\",\n",
    "        \"intensity_spread\", \"intensity_spread_compensated\",\n",
    "        \"spread\", \"liq_ratio\",\n",
    "        \"OFI_inc\", \"OFI_cum_short\", \"OFI_cum_long\", \"QI\",\n",
    "        \"mid_price\", \"price_off_open\",\n",
    "        \"price_mov_derivative_s\", \"vol_s\", \"momentum_s\",\n",
    "        \"price_mov_derivative_l\", \"vol_l\", \"momentum_l\",\n",
    "        \"t_cur\",\n",
    "    ]\n",
    "    raw_features_df = pd.DataFrame(feature_hist_arr, columns=feature_names)\n",
    "\n",
    "    # drop rows with NaNs\n",
    "    raw_features_df = raw_features_df[~raw_features_df.isnull().any(axis=1)].copy()\n",
    "\n",
    "    # sort by time ON THE MAIN DF\n",
    "    raw_features_df = raw_features_df.sort_values(\"t_cur\").reset_index(drop=True)\n",
    "\n",
    "    # Create the actual output features, compute the average of the NEXT 5s mid price \n",
    "    raw_features_df[\"avg_5_ticks\"] = (\n",
    "        raw_features_df[\"mid_price\"]\n",
    "        .shift(-1)       # start at t+1\n",
    "        .iloc[::-1]\n",
    "        .rolling(5)      # 5 future ticks\n",
    "        .mean()\n",
    "        .iloc[::-1]\n",
    "    )\n",
    "\n",
    "    # Get 10 second average \n",
    "    raw_features_df[\"avg_10_ticks\"] = (\n",
    "        raw_features_df[\"mid_price\"]\n",
    "        .shift(-1)       # start at t+1\n",
    "        .iloc[::-1]\n",
    "        .rolling(10)      # 10 future ticks\n",
    "        .mean()\n",
    "        .iloc[::-1]\n",
    "    )\n",
    "\n",
    "    # Drop NA \n",
    "    raw_features_df = raw_features_df[~raw_features_df[\"avg_5_ticks\"].isna()].reset_index(drop=True)\n",
    "    raw_features_df = raw_features_df[~raw_features_df[\"avg_10_ticks\"].isna()].reset_index(drop=True)\n",
    "\n",
    "    # Compute the delta between the current price, and the next 5 seconds average + again for 10 second average \n",
    "    raw_features_df[\"5s_price_delta\"] = raw_features_df[\"avg_5_ticks\"] - raw_features_df[\"mid_price\"] \n",
    "    raw_features_df[\"10s_price_delta\"] = raw_features_df[\"avg_10_ticks\"] - raw_features_df[\"mid_price\"] \n",
    "\n",
    "    return raw_features_df.reset_index(drop=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aea48d",
   "metadata": {},
   "source": [
    "# Dataframe construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff6a6200",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_training = \"dataset_bb0_1/APPL_MIX_1s_training\"\n",
    "root_validation = \"dataset_bb0_1/APPL_MIX_1s_validation\"\n",
    "start_idx = 50\n",
    "hour_per_day = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "babfe2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06666666666666667\n",
      "0.13333333333333333\n",
      "0.2\n",
      "0.26666666666666666\n",
      "0.3333333333333333\n",
      "0.4\n",
      "0.4666666666666667\n",
      "0.5333333333333333\n",
      "0.6\n",
      "0.6666666666666666\n",
      "0.7333333333333333\n",
      "0.8\n",
      "0.8666666666666667\n",
      "0.9333333333333333\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "daily_feature_list = []\n",
    "\n",
    "num_files = len(os.listdir(root_training))\n",
    "for j, filename in enumerate(os.listdir(root_training)):\n",
    "    path = f\"{root_training}/{filename}\"\n",
    "    symbol_day = filename.split(\".\")[0]\n",
    "\n",
    "    # Raw per-day feed\n",
    "    raw_day_df = pd.read_csv(path, compression='zstd')\n",
    "\n",
    "    # Define hawkes system \n",
    "    hwk = hawkes_system()\n",
    "\n",
    "    end_idx = min(start_idx + 3600*hour_per_day, len(raw_day_df)-1)\n",
    "\n",
    "    for i in range(start_idx, end_idx):\n",
    "        row = raw_day_df.loc[[i], :]\n",
    "        hwk.read_pd_raw(row_curr=row)\n",
    "        hwk.update_features()\n",
    "\n",
    "    # Per-day feature frame\n",
    "    day_features = get_feature_frames(hwk)\n",
    "    day_features.to_csv(f\"dataset_ml/training_data_raw/{symbol_day}.csv\")\n",
    "\n",
    "    daily_feature_list.append(day_features)\n",
    "\n",
    "    print(np.round((j+1)/num_files, 2))\n",
    "\n",
    "# Final merge\n",
    "full_training_df = pd.concat(daily_feature_list, ignore_index=True)\n",
    "full_training_df.to_csv(\"dataset_ml/training_data_hwks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a275cef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14\n",
      "0.29\n",
      "0.43\n",
      "0.57\n",
      "0.71\n",
      "0.86\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "daily_feature_list = []\n",
    "\n",
    "num_files = len(os.listdir(root_validation))\n",
    "for j, filename in enumerate(os.listdir(root_validation)):\n",
    "    path = f\"{root_validation}/{filename}\"\n",
    "    symbol_day = filename.split(\".\")[0]\n",
    "\n",
    "    # Raw per-day feed\n",
    "    raw_day_df = pd.read_csv(path, compression='zstd')\n",
    "\n",
    "    # Define hawkes system \n",
    "    hwk = hawkes_system()\n",
    "\n",
    "    end_idx = min(start_idx + 3600*hour_per_day, len(raw_day_df)-1)\n",
    "\n",
    "    for i in range(start_idx, end_idx):\n",
    "        row = raw_day_df.loc[[i], :]\n",
    "        hwk.read_pd_raw(row_curr=row)\n",
    "        hwk.update_features()\n",
    "\n",
    "    # Per-day feature frame\n",
    "    day_features = get_feature_frames(hwk)\n",
    "    day_features.to_csv(f\"dataset_ml/validation_data_raw/{symbol_day}.csv\")\n",
    "\n",
    "    daily_feature_list.append(day_features)\n",
    "\n",
    "    print(np.round((j+1)/num_files, 2))\n",
    "\n",
    "# Final merge\n",
    "full_validation_df = pd.concat(daily_feature_list, ignore_index=True)\n",
    "full_validation_df.to_csv(\"dataset_ml/validation_data_hwks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40497da",
   "metadata": {},
   "source": [
    "# Baseline Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "8de0e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('dataset_ml/training_data_hwks.csv', index_col =0)\n",
    "validation_data = pd.read_csv('dataset_ml/validation_data_hwks.csv', index_col =0)\n",
    "\n",
    "training_data = training_data[training_data['intensity_spread_compensated'] != 0]\n",
    "training_data = training_data[training_data['lam_comp_2'] !=0]  \n",
    "training_data.drop(['mid_price',  'price_off_open'], axis = 1, inplace =True)\n",
    "\n",
    "validation_data = validation_data[validation_data['intensity_spread_compensated'] != 0]\n",
    "validation_data= validation_data[validation_data['lam_comp_2'] !=0]  \n",
    "validation_data.drop(['mid_price',  'price_off_open'], axis = 1, inplace =True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27657168",
   "metadata": {},
   "source": [
    "## 5s Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "c8a62085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy so we don't mutate your originals\n",
    "train = training_data.copy()\n",
    "valid = validation_data.copy()\n",
    "\n",
    "drop_col = ['lam_0', 'lam_1', 'intensity_spread', 't_cur', 'spread', 'intensity_spread_compensated']\n",
    "hawkes_cols = ['lam_comp_2', 'lam_comp_0', 'lam_comp_1']\n",
    "target_col = '10s_price_delta'\n",
    "\n",
    "raw_feature_col = ['avg_5_ticks', 'avg_10_ticks', '5s_price_delta', '10s_price_delta']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "8ea9b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def run_linear_model(feature_cols, name):\n",
    "    # Select features\n",
    "    X_train = train[feature_cols].copy()\n",
    "    y_train = train[target_col]\n",
    "\n",
    "    X_val = valid[feature_cols].copy()\n",
    "    y_val = valid[target_col]\n",
    "    \n",
    "    # -------- Standard scaling (no leakage) --------\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)   # fit only on train\n",
    "    X_val_scaled   = scaler.transform(X_val)         # apply to validation\n",
    "\n",
    "    # -------- Linear model --------\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    pred = model.predict(X_val_scaled)\n",
    "    mse = mean_squared_error(y_val, pred)\n",
    "    r2 = r2_score(y_val, pred)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Num features:\", len(feature_cols))\n",
    "    print(\"MSE:\", mse)\n",
    "    print(\"R²:\", r2)\n",
    "\n",
    "    return model, scaler, mse, r2\n",
    "\n",
    "def run_gb_model(feature_cols, name):\n",
    "    # Select features\n",
    "    X_train = train[feature_cols].copy().values\n",
    "    y_train = train[target_col].values\n",
    "\n",
    "    X_val = valid[feature_cols].copy().values\n",
    "    y_val = valid[target_col].values\n",
    "\n",
    "    # -------- Gradient boosting regressor (non-linear) --------\n",
    "    model = HistGradientBoostingRegressor(\n",
    "        max_depth=6,             # allow some complexity, but not crazy\n",
    "        learning_rate=0.05,\n",
    "        max_iter=300,\n",
    "        min_samples_leaf=50,     # regularization against noise\n",
    "        l2_regularization=1.0,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    pred = model.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, pred)\n",
    "    r2 = r2_score(y_val, pred)\n",
    "\n",
    "    print(f\"\\n=== {name} (HistGradientBoostingRegressor) ===\")\n",
    "    print(\"Num features:\", len(feature_cols))\n",
    "    print(\"MSE:\", mse)\n",
    "    print(\"R²:\", r2)\n",
    "\n",
    "    return model, mse, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "1122a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model A (no drop_col features) ===\n",
      "Num features: 15\n",
      "MSE: 0.004177257601573856\n",
      "R²: 0.24614611928647456\n",
      "\n",
      "=== Model A (no drop_col features) (HistGradientBoostingRegressor) ===\n",
      "Num features: 15\n",
      "MSE: 0.004472008643916497\n",
      "R²: 0.1929535134412811\n"
     ]
    }
   ],
   "source": [
    "feature_cols_A = [c for c in train.columns \n",
    "                  if c not in drop_col  \n",
    "                  and c != target_col\n",
    "                  and c not in raw_feature_col]\n",
    "\n",
    "model, scaler, mse, r2 = run_linear_model(feature_cols_A, \n",
    "                                        \"Model A (no drop_col features)\")\n",
    "\n",
    "model, mse, r2 = run_gb_model(feature_cols_A, \"Model A (no drop_col features)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "401291e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>max_abs_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lam_2</td>\n",
       "      <td>5.946204e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lam_comp_0</td>\n",
       "      <td>6.261809e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lam_comp_1</td>\n",
       "      <td>-1.407918e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lam_comp_2</td>\n",
       "      <td>-6.930260e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>liq_ratio</td>\n",
       "      <td>-1.491801e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OFI_inc</td>\n",
       "      <td>2.611888e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OFI_cum_short</td>\n",
       "      <td>3.788549e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OFI_cum_long</td>\n",
       "      <td>2.095459e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>QI</td>\n",
       "      <td>-8.063120e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>price_mov_derivative_s</td>\n",
       "      <td>8.652947e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>vol_s</td>\n",
       "      <td>7.858093e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>momentum_s</td>\n",
       "      <td>-2.108294e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>price_mov_derivative_l</td>\n",
       "      <td>3.008959e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vol_l</td>\n",
       "      <td>7.856426e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>momentum_l</td>\n",
       "      <td>-1.654932e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feature  max_abs_coef\n",
       "0                    lam_2  5.946204e-02\n",
       "1               lam_comp_0  6.261809e-03\n",
       "2               lam_comp_1 -1.407918e-02\n",
       "3               lam_comp_2 -6.930260e-03\n",
       "4                liq_ratio -1.491801e-04\n",
       "5                  OFI_inc  2.611888e-07\n",
       "6            OFI_cum_short  3.788549e-06\n",
       "7             OFI_cum_long  2.095459e-07\n",
       "8                       QI -8.063120e-03\n",
       "9   price_mov_derivative_s  8.652947e-02\n",
       "10                   vol_s  7.858093e-03\n",
       "11              momentum_s -2.108294e-01\n",
       "12  price_mov_derivative_l  3.008959e+00\n",
       "13                   vol_l  7.856426e-03\n",
       "14              momentum_l -1.654932e-01"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols_A,\n",
    "    \"max_abs_coef\": model_A.coef_,\n",
    "})\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "8286f676",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\anaconda3\\envs\\hawkes\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'dir'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[360]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m feature_cols_B = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m train.columns\n\u001b[32m      2\u001b[39m                   \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m hawkes_cols\n\u001b[32m      3\u001b[39m                   \u001b[38;5;129;01mand\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m drop_col\n\u001b[32m      4\u001b[39m                   \u001b[38;5;129;01mand\u001b[39;00m c != target_col\n\u001b[32m      5\u001b[39m                   \u001b[38;5;129;01mand\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m raw_feature_col]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model_B, mse_B, r2_B = \u001b[43mrun_linear_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_cols_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                                        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mModel B (no Hawkes features)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[286]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mrun_linear_model\u001b[39m\u001b[34m(feature_cols, name)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_linear_model\u001b[39m(feature_cols, name):\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Select features\u001b[39;00m\n\u001b[32m      6\u001b[39m     X_train = train[feature_cols].copy()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     y_train = \u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      9\u001b[39m     X_val = valid[feature_cols].copy()\n\u001b[32m     10\u001b[39m     y_val = valid[target_col]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\anaconda3\\envs\\hawkes\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\isaac\\anaconda3\\envs\\hawkes\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'dir'"
     ]
    }
   ],
   "source": [
    "feature_cols_B = [c for c in train.columns\n",
    "                  if c not in hawkes_cols\n",
    "                  and c not in drop_col\n",
    "                  and c != target_col\n",
    "                  and c not in raw_feature_col]\n",
    "\n",
    "model_B, mse_B, r2_B = run_linear_model(feature_cols_B, \n",
    "                                        \"Model B (no Hawkes features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "6d075aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model C (Hawkes-only) ===\n",
      "Num features: 3\n",
      "MSE: 0.005536717971846841\n",
      "R^2: 0.0008094478251763348\n"
     ]
    }
   ],
   "source": [
    "feature_cols_C = hawkes_cols\n",
    "\n",
    "model_C, mse_C, r2_C = run_linear_model(feature_cols_C,\n",
    "                                        \"Model C (Hawkes-only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece89a4",
   "metadata": {},
   "source": [
    "## 5s classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "23f81e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = .03\n",
    "drop_col = ['lam_0', 'lam_1', 'lam_comp_2', 'intensity_spread', \n",
    "            't_cur', 'spread', 'intensity_spread_compensated', 'momentum_s', 'vol_l', \n",
    "            'OFI_cum_short', 'OFI_inc', 'momentum_l', 'price_mov_derivative_s', 'OFI_cum_long', 'price_mov_derivative_l']\n",
    "raw_feature_col = ['avg_5_ticks', 'avg_10_ticks', '5s_price_delta', '10s_price_delta']\n",
    "target_col = 'dir'\n",
    "\n",
    "\n",
    "\n",
    "training_data_5s = training_data.copy()\n",
    "validation_data_5s = validation_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "90f89b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.2, 0.2)"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = training_data_5s['10s_price_delta'].hist(bins=500)\n",
    "ax.set_xlim([-0.2, 0.2])   # example limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203ab46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir\n",
      "0    179242\n",
      "1     79593\n",
      "Name: count, dtype: int64\n",
      "dir\n",
      "0    76225\n",
      "1    42559\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_data_5s['dir'] = 0 \n",
    "training_data_5s.loc[training_data_5s['10s_price_delta'] > epsilon , 'dir'] = 1 \n",
    "training_data_5s.loc[training_data_5s['10s_price_delta'] < -epsilon , 'dir'] = 1\n",
    "\n",
    "validation_data_5s['dir'] = 0 \n",
    "validation_data_5s.loc[validation_data_5s['10s_price_delta'] > epsilon , 'dir'] = 1 \n",
    "validation_data_5s.loc[validation_data_5s['10s_price_delta'] < -epsilon , 'dir'] = 1\n",
    "\n",
    "training_data_5s = training_data_5s.drop(drop_col + raw_feature_col, axis =1).sample(frac=1)\n",
    "validation_data_5s = validation_data_5s.drop(drop_col + raw_feature_col, axis =1).sample(frac=1)\n",
    "\n",
    "print(training_data_5s['dir'].value_counts())\n",
    "print(validation_data_5s['dir'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a6057",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "d1253154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = training_data_5s.corr()\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    annot=False,\n",
    "    cmap=\"coolwarm\",\n",
    "    linewidths=0.3,\n",
    "    center=0\n",
    ")\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7799fea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vol_s', 'vol_l', np.float64(0.8415442214960177)),\n",
       " ('price_mov_derivative_l', 'momentum_l', np.float64(0.8455897655941423))]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = .8\n",
    "high_corr = []\n",
    "cols = corr.columns\n",
    "\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i+1, len(cols)):   # avoid duplicates + diagonal\n",
    "        c = corr.iloc[i, j]\n",
    "        if abs(c) > threshold:\n",
    "            high_corr.append((cols[i], cols[j], c))\n",
    "\n",
    "high_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54449824",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "e4062e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\AppData\\Local\\Temp\\ipykernel_13800\\1553969709.py:7: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(min_count, replace=False))\n"
     ]
    }
   ],
   "source": [
    "counts = training_data_5s[target_col].value_counts()\n",
    "min_count = counts.min()   # smallest class count\n",
    "\n",
    "# For each class, randomly sample min_count rows\n",
    "training_data_5s = (\n",
    "    training_data_5s.groupby(target_col)\n",
    "        .apply(lambda x: x.sample(min_count, replace=False))\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "# training_data_5s = training_data_5s[training_data_5s['dir'] !=0]\n",
    "# validation_data_5s = validation_data_5s[validation_data_5s['dir'] !=0]\n",
    "\n",
    "\n",
    "# training_data_5s['dir'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "2908e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = validation_data_5s.drop(target_col, axis =1 )\n",
    "Y_train = validation_data_5s[target_col]\n",
    "\n",
    "X_test = training_data_5s.drop(target_col, axis =1 )\n",
    "Y_test = training_data_5s[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff52611",
   "metadata": {},
   "source": [
    "Test two classifiers, with and without the hawkes features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "3eac319a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== L1 Logistic Regression ===\n",
      "Overall accuracy: 0.6606611134144963\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.78      0.70     79593\n",
      "           1       0.71      0.54      0.61     79593\n",
      "\n",
      "    accuracy                           0.66    159186\n",
      "   macro avg       0.67      0.66      0.66    159186\n",
      "weighted avg       0.67      0.66      0.66    159186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If X_train is a DataFrame, get feature names from columns\n",
    "if hasattr(X_train, \"columns\"):\n",
    "    feature_names = X_train.columns.tolist()\n",
    "else:\n",
    "    # otherwise you need to define this list yourself\n",
    "    feature_names = [f\"f_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    solver=\"saga\",\n",
    "    C=0.1,                 # stronger regularization -> more zeros\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=5000,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "log_reg_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),   # fit ONLY on training data\n",
    "    (\"log_reg\", log_reg),\n",
    "])\n",
    "\n",
    "# Fit on train\n",
    "log_reg_pipe.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on test\n",
    "y_pred_lr = log_reg_pipe.predict(X_test)\n",
    "\n",
    "print(\"\\n=== L1 Logistic Regression ===\")\n",
    "print(\"Overall accuracy:\", accuracy_score(Y_test, y_pred_lr))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(Y_test, y_pred_lr, digits=2))\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Inspect feature coefficients (importance)\n",
    "# -------------------------------\n",
    "lr = log_reg_pipe.named_steps[\"log_reg\"]\n",
    "coefs = lr.coef_  # shape: (n_classes, n_features)\n",
    "\n",
    "# aggregate importance per feature as max abs coef across classes\n",
    "max_abs_coef = np.max(np.abs(coefs), axis=0)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"max_abs_coef\": max_abs_coef,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "3e78db1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>max_abs_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vol_s</td>\n",
       "      <td>0.589466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lam_comp_0</td>\n",
       "      <td>0.270826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lam_comp_1</td>\n",
       "      <td>0.235562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lam_2</td>\n",
       "      <td>0.227393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>liq_ratio</td>\n",
       "      <td>0.051760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QI</td>\n",
       "      <td>0.046384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  max_abs_coef\n",
       "5       vol_s      0.589466\n",
       "1  lam_comp_0      0.270826\n",
       "2  lam_comp_1      0.235562\n",
       "0       lam_2      0.227393\n",
       "3   liq_ratio      0.051760\n",
       "4          QI      0.046384"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values(by='max_abs_coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "53f655c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== L1 Logistic Regression ===\n",
      "Overall accuracy: 0.5553609913793104\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.81      0.52     34865\n",
      "           1       0.85      0.45      0.59     83919\n",
      "\n",
      "    accuracy                           0.56    118784\n",
      "   macro avg       0.61      0.63      0.55    118784\n",
      "weighted avg       0.71      0.56      0.57    118784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = training_data_5s.drop([target_col] + ['lam_2', 'lam_comp_0', 'lam_comp_1'], axis =1 )\n",
    "Y_train = training_data_5s[target_col]\n",
    "\n",
    "X_test = validation_data_5s.drop([target_col]  + ['lam_2', 'lam_comp_0', 'lam_comp_1'], axis =1 )\n",
    "Y_test = validation_data_5s[target_col]\n",
    "\n",
    "# feature names \n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    solver=\"saga\",\n",
    "    C=0.1,                 # stronger regularization -> more zeros\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=5000,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "log_reg_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),   # fit ONLY on training data\n",
    "    (\"log_reg\", log_reg),\n",
    "])\n",
    "\n",
    "# Fit on train\n",
    "log_reg_pipe.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on test\n",
    "y_pred_lr = log_reg_pipe.predict(X_test)\n",
    "\n",
    "print(\"\\n=== L1 Logistic Regression ===\")\n",
    "print(\"Overall accuracy:\", accuracy_score(Y_test, y_pred_lr))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(Y_test, y_pred_lr, digits=2))\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Inspect feature coefficients (importance)\n",
    "# -------------------------------\n",
    "lr = log_reg_pipe.named_steps[\"log_reg\"]\n",
    "coefs = lr.coef_  # shape: (n_classes, n_features)\n",
    "\n",
    "# aggregate importance per feature as max abs coef across classes\n",
    "max_abs_coef = np.max(np.abs(coefs), axis=0)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"max_abs_coef\": max_abs_coef,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "8f8e486b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>max_abs_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vol_s</td>\n",
       "      <td>0.746623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>liq_ratio</td>\n",
       "      <td>0.048690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QI</td>\n",
       "      <td>0.008423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature  max_abs_coef\n",
       "2      vol_s      0.746623\n",
       "0  liq_ratio      0.048690\n",
       "1         QI      0.008423"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values(by='max_abs_coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "8fda5e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== L1 Logistic Regression ===\n",
      "Overall accuracy: 0.6607034617456896\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.49      0.53     45741\n",
      "           1       0.71      0.77      0.74     73043\n",
      "\n",
      "    accuracy                           0.66    118784\n",
      "   macro avg       0.64      0.63      0.63    118784\n",
      "weighted avg       0.65      0.66      0.66    118784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = training_data_5s.drop([target_col] + ['vol_s'], axis =1 )\n",
    "Y_train = training_data_5s[target_col]\n",
    "\n",
    "X_test = validation_data_5s.drop([target_col]  + ['vol_s'], axis =1 )\n",
    "Y_test = validation_data_5s[target_col]\n",
    "\n",
    "# feature names \n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    solver=\"saga\",\n",
    "    C=0.1,                 # stronger regularization -> more zeros\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=5000,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "log_reg_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),   # fit ONLY on training data\n",
    "    (\"log_reg\", log_reg),\n",
    "])\n",
    "\n",
    "# Fit on train\n",
    "log_reg_pipe.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on test\n",
    "y_pred_lr = log_reg_pipe.predict(X_test)\n",
    "\n",
    "print(\"\\n=== L1 Logistic Regression ===\")\n",
    "print(\"Overall accuracy:\", accuracy_score(Y_test, y_pred_lr))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(Y_test, y_pred_lr, digits=2))\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Inspect feature coefficients (importance)\n",
    "# -------------------------------\n",
    "lr = log_reg_pipe.named_steps[\"log_reg\"]\n",
    "coefs = lr.coef_  # shape: (n_classes, n_features)\n",
    "\n",
    "# aggregate importance per feature as max abs coef across classes\n",
    "max_abs_coef = np.max(np.abs(coefs), axis=0)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"max_abs_coef\": max_abs_coef,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "1ee3f927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.65      0.67      0.66     12647\n",
      "           1       0.65      0.63      0.64     12117\n",
      "\n",
      "    accuracy                           0.65     24764\n",
      "   macro avg       0.65      0.65      0.65     24764\n",
      "weighted avg       0.65      0.65      0.65     24764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "clf_3class = HistGradientBoostingClassifier(\n",
    "    max_depth=None,\n",
    "    learning_rate=0.05,\n",
    "    max_iter=400,\n",
    ")\n",
    "\n",
    "clf_3class.fit(X_train, Y_train)\n",
    "y_pred_3 = clf_3class.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, y_pred_3, digits=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68a5c8",
   "metadata": {},
   "source": [
    "# Dual Modeling Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "2436ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "2271d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('dataset_ml/training_data_hwks.csv', index_col =0)\n",
    "validation_data = pd.read_csv('dataset_ml/validation_data_hwks.csv', index_col =0)\n",
    "\n",
    "training_data = training_data[training_data['intensity_spread_compensated'] != 0]\n",
    "training_data = training_data[training_data['lam_comp_1'] !=0]  \n",
    "training_data.drop(['mid_price',  'price_off_open'], axis = 1, inplace =True)\n",
    "\n",
    "validation_data = validation_data[validation_data['intensity_spread_compensated'] != 0]\n",
    "validation_data= validation_data[validation_data['lam_comp_1'] !=0]  \n",
    "validation_data.drop(['mid_price',  'price_off_open'], axis = 1, inplace =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "d3ee1836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_under_sample(df, target_col): \n",
    "    counts = df[target_col].value_counts()\n",
    "    min_count = counts.min()   # smallest class count\n",
    "\n",
    "    # For each class, randomly sample min_count rows\n",
    "    return (\n",
    "        df.groupby(target_col)\n",
    "            .apply(lambda x: x.sample(min_count, replace=False))\n",
    "            .reset_index(drop=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b2e30c",
   "metadata": {},
   "source": [
    "Plan: Train a classifier with hawkes features + volality to model at different epislon values, then train a new model with the rest of the features on price up down direction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "d9c4d281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lam_0</th>\n",
       "      <th>lam_1</th>\n",
       "      <th>lam_2</th>\n",
       "      <th>lam_comp_0</th>\n",
       "      <th>lam_comp_1</th>\n",
       "      <th>lam_comp_2</th>\n",
       "      <th>intensity_spread</th>\n",
       "      <th>intensity_spread_compensated</th>\n",
       "      <th>spread</th>\n",
       "      <th>liq_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>momentum_s</th>\n",
       "      <th>price_mov_derivative_l</th>\n",
       "      <th>vol_l</th>\n",
       "      <th>momentum_l</th>\n",
       "      <th>t_cur</th>\n",
       "      <th>avg_5_ticks</th>\n",
       "      <th>avg_10_ticks</th>\n",
       "      <th>5s_price_delta</th>\n",
       "      <th>10s_price_delta</th>\n",
       "      <th>dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.252887</td>\n",
       "      <td>0.262271</td>\n",
       "      <td>0.029263</td>\n",
       "      <td>0.035231</td>\n",
       "      <td>0.023549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009384</td>\n",
       "      <td>0.011682</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>1.064541</td>\n",
       "      <td>-0.605</td>\n",
       "      <td>751.0</td>\n",
       "      <td>171.824</td>\n",
       "      <td>171.9750</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.253563</td>\n",
       "      <td>0.254012</td>\n",
       "      <td>0.028233</td>\n",
       "      <td>0.037657</td>\n",
       "      <td>0.022808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000449</td>\n",
       "      <td>0.014849</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.125</td>\n",
       "      <td>...</td>\n",
       "      <td>1.615</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>1.146287</td>\n",
       "      <td>1.750</td>\n",
       "      <td>765.0</td>\n",
       "      <td>171.821</td>\n",
       "      <td>171.9745</td>\n",
       "      <td>-1.564</td>\n",
       "      <td>-1.4105</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.257497</td>\n",
       "      <td>0.267164</td>\n",
       "      <td>0.031629</td>\n",
       "      <td>0.038241</td>\n",
       "      <td>0.023463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009667</td>\n",
       "      <td>0.014778</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>1.179091</td>\n",
       "      <td>0.785</td>\n",
       "      <td>768.0</td>\n",
       "      <td>172.140</td>\n",
       "      <td>171.9740</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.1740</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.245550</td>\n",
       "      <td>0.261171</td>\n",
       "      <td>0.026524</td>\n",
       "      <td>0.036467</td>\n",
       "      <td>0.024017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.015622</td>\n",
       "      <td>0.012449</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.007849</td>\n",
       "      <td>1.180807</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>782.0</td>\n",
       "      <td>172.276</td>\n",
       "      <td>172.0435</td>\n",
       "      <td>1.181</td>\n",
       "      <td>0.9485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.241793</td>\n",
       "      <td>0.258538</td>\n",
       "      <td>0.024804</td>\n",
       "      <td>0.035909</td>\n",
       "      <td>0.023691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.016745</td>\n",
       "      <td>0.012217</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.610</td>\n",
       "      <td>-0.017181</td>\n",
       "      <td>1.180799</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>798.0</td>\n",
       "      <td>172.270</td>\n",
       "      <td>172.1165</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1.0465</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125610</th>\n",
       "      <td>0.349716</td>\n",
       "      <td>0.360325</td>\n",
       "      <td>0.092068</td>\n",
       "      <td>0.374039</td>\n",
       "      <td>0.427197</td>\n",
       "      <td>0.016033</td>\n",
       "      <td>-0.010608</td>\n",
       "      <td>-0.053158</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.150</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.000517</td>\n",
       "      <td>0.154451</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>18789.0</td>\n",
       "      <td>274.022</td>\n",
       "      <td>274.0015</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125611</th>\n",
       "      <td>0.357937</td>\n",
       "      <td>0.360472</td>\n",
       "      <td>0.093654</td>\n",
       "      <td>0.373645</td>\n",
       "      <td>0.427371</td>\n",
       "      <td>0.016309</td>\n",
       "      <td>-0.002535</td>\n",
       "      <td>-0.053726</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.155163</td>\n",
       "      <td>0.040</td>\n",
       "      <td>18790.0</td>\n",
       "      <td>274.001</td>\n",
       "      <td>273.9955</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.0595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125612</th>\n",
       "      <td>0.356226</td>\n",
       "      <td>0.366558</td>\n",
       "      <td>0.092798</td>\n",
       "      <td>0.371859</td>\n",
       "      <td>0.422412</td>\n",
       "      <td>0.016160</td>\n",
       "      <td>-0.010332</td>\n",
       "      <td>-0.050553</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.154856</td>\n",
       "      <td>0.075</td>\n",
       "      <td>18791.0</td>\n",
       "      <td>273.985</td>\n",
       "      <td>273.9960</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.0390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125613</th>\n",
       "      <td>0.354677</td>\n",
       "      <td>0.372065</td>\n",
       "      <td>0.092022</td>\n",
       "      <td>0.370243</td>\n",
       "      <td>0.440323</td>\n",
       "      <td>0.016025</td>\n",
       "      <td>-0.017388</td>\n",
       "      <td>-0.070081</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.154542</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18792.0</td>\n",
       "      <td>273.982</td>\n",
       "      <td>273.9965</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.0335</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125614</th>\n",
       "      <td>0.353276</td>\n",
       "      <td>0.377048</td>\n",
       "      <td>0.091321</td>\n",
       "      <td>0.368780</td>\n",
       "      <td>0.515156</td>\n",
       "      <td>0.015903</td>\n",
       "      <td>-0.023772</td>\n",
       "      <td>-0.146375</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.150</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>0.154863</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>18793.0</td>\n",
       "      <td>273.981</td>\n",
       "      <td>274.0020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125535 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lam_0     lam_1     lam_2  lam_comp_0  lam_comp_1  lam_comp_2  \\\n",
       "16      0.252887  0.262271  0.029263    0.035231    0.023549    0.000000   \n",
       "17      0.253563  0.254012  0.028233    0.037657    0.022808    0.000000   \n",
       "18      0.257497  0.267164  0.031629    0.038241    0.023463    0.000000   \n",
       "19      0.245550  0.261171  0.026524    0.036467    0.024017    0.000000   \n",
       "20      0.241793  0.258538  0.024804    0.035909    0.023691    0.000000   \n",
       "...          ...       ...       ...         ...         ...         ...   \n",
       "125610  0.349716  0.360325  0.092068    0.374039    0.427197    0.016033   \n",
       "125611  0.357937  0.360472  0.093654    0.373645    0.427371    0.016309   \n",
       "125612  0.356226  0.366558  0.092798    0.371859    0.422412    0.016160   \n",
       "125613  0.354677  0.372065  0.092022    0.370243    0.440323    0.016025   \n",
       "125614  0.353276  0.377048  0.091321    0.368780    0.515156    0.015903   \n",
       "\n",
       "        intensity_spread  intensity_spread_compensated  spread  liq_ratio  \\\n",
       "16             -0.009384                      0.011682    1.43      1.000   \n",
       "17             -0.000449                      0.014849    3.23      3.125   \n",
       "18             -0.009667                      0.014778    0.06      1.000   \n",
       "19             -0.015622                      0.012449    1.47      1.000   \n",
       "20             -0.016745                      0.012217    1.42      1.000   \n",
       "...                  ...                           ...     ...        ...   \n",
       "125610         -0.010608                     -0.053158    0.17      1.150   \n",
       "125611         -0.002535                     -0.053726    0.03      4.000   \n",
       "125612         -0.010332                     -0.050553    0.03      0.500   \n",
       "125613         -0.017388                     -0.070081    0.04      0.500   \n",
       "125614         -0.023772                     -0.146375    0.14      1.150   \n",
       "\n",
       "        ...  momentum_s  price_mov_derivative_l     vol_l  momentum_l  \\\n",
       "16      ...       0.010                0.003399  1.064541      -0.605   \n",
       "17      ...       1.615                0.010146  1.146287       1.750   \n",
       "18      ...       0.740                0.005597  1.179091       0.785   \n",
       "19      ...       0.020               -0.007849  1.180807      -0.545   \n",
       "20      ...      -1.610               -0.017181  1.180799      -0.575   \n",
       "...     ...         ...                     ...       ...         ...   \n",
       "125610  ...      -0.030               -0.000517  0.154451      -0.040   \n",
       "125611  ...       0.050                0.000442  0.155163       0.040   \n",
       "125612  ...       0.030                0.001254  0.154856       0.075   \n",
       "125613  ...       0.075                0.001645  0.154542       0.015   \n",
       "125614  ...      -0.045                0.001933  0.154863      -0.010   \n",
       "\n",
       "          t_cur  avg_5_ticks  avg_10_ticks  5s_price_delta  10s_price_delta  \\\n",
       "16        751.0      171.824      171.9750           0.749           0.9000   \n",
       "17        765.0      171.821      171.9745          -1.564          -1.4105   \n",
       "18        768.0      172.140      171.9740           0.340           0.1740   \n",
       "19        782.0      172.276      172.0435           1.181           0.9485   \n",
       "20        798.0      172.270      172.1165           1.200           1.0465   \n",
       "...         ...          ...           ...             ...              ...   \n",
       "125610  18789.0      274.022      274.0015           0.047           0.0265   \n",
       "125611  18790.0      274.001      273.9955          -0.054          -0.0595   \n",
       "125612  18791.0      273.985      273.9960          -0.050          -0.0390   \n",
       "125613  18792.0      273.982      273.9965          -0.048          -0.0335   \n",
       "125614  18793.0      273.981      274.0020           0.021           0.0420   \n",
       "\n",
       "        dir  \n",
       "16        1  \n",
       "17       -1  \n",
       "18        1  \n",
       "19        1  \n",
       "20        1  \n",
       "...     ...  \n",
       "125610    0  \n",
       "125611    0  \n",
       "125612    0  \n",
       "125613    0  \n",
       "125614    0  \n",
       "\n",
       "[125535 rows x 26 columns]"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "03770e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\AppData\\Local\\Temp\\ipykernel_13800\\562661461.py:8: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(min_count, replace=False))\n"
     ]
    }
   ],
   "source": [
    "epsilon = .02\n",
    "dir_indicator = '5s_price_delta'\n",
    "\n",
    "cols_hawks = ['lam_comp_0', 'lam_comp_1', 'lam_2']\n",
    "cols_other_vol = ['vol_l', 'spread', 'liq_ratio']\n",
    "cols_volitility = cols_hawks + cols_other_vol\n",
    "\n",
    "cols_direction = ['OFI_inc', 'OFI_cum_long', 'QI', 'liq_ratio','momentum_s', 'lam_2', 'spread', 'price_mov_derivative_l', 'lam_comp_0', 'lam_comp_1'\n",
    "                  ]\n",
    "\n",
    "# Copy dataframes \n",
    "training_data_temp = training_data.copy()\n",
    "validation_data_temp = validation_data.copy()\n",
    "\n",
    "# Add direction \n",
    "training_data_temp['dir'] = 0 \n",
    "training_data_temp.loc[training_data_temp[dir_indicator] > epsilon , 'dir'] = 1 \n",
    "training_data_temp.loc[training_data_temp[dir_indicator] < -epsilon , 'dir'] = -1\n",
    "\n",
    "validation_data_temp['dir'] = 0 \n",
    "validation_data_temp.loc[validation_data_temp[dir_indicator] > epsilon , 'dir'] = 1 \n",
    "validation_data_temp.loc[validation_data_temp[dir_indicator] < -epsilon , 'dir'] = -1\n",
    "\n",
    "# define volality dataset \n",
    "training_data_vol = training_data_temp[cols_volitility + ['dir']].sample(frac=1)\n",
    "training_data_vol.loc[training_data_vol['dir'] == -1, 'dir' ] =  1\n",
    "training_data_vol = quick_under_sample(training_data_vol, 'dir')\n",
    "\n",
    "validation_data_vol = validation_data_temp.loc[:, cols_volitility + ['dir']].sample(frac=1)\n",
    "validation_data_vol.loc[validation_data_vol['dir'] == -1, 'dir' ] =  1\n",
    "\n",
    "# Define direction dataset \n",
    "training_data_dir = training_data_temp.loc[:, cols_direction + ['dir']].sample(frac=1)\n",
    "training_data_dir = training_data_dir.loc[training_data_dir['dir'] != 0, ]\n",
    "\n",
    "validation_data_dir = validation_data_temp.loc[:, cols_direction + ['dir']].sample(frac=1)\n",
    "validation_data_dir = validation_data_dir.loc[validation_data_dir['dir'] != 0, ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682992e2",
   "metadata": {},
   "source": [
    "For model 1 we want a precision focused model, TP/ (TP + FP) high false positives mean we enter the market when we shouldn't costing fees and time. \n",
    "For model 2 we care about overal accuracy, since false postives and false negatives are equally bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "8b3cf48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log_reg(df_t, df_v, target_col='dir', threshold=0.5):\n",
    "    # Split features / target\n",
    "    X_t = df_t.drop(columns=[target_col])\n",
    "    y_t = df_t[target_col]\n",
    "\n",
    "    X_v = df_v.drop(columns=[target_col])\n",
    "    y_v = df_v[target_col]\n",
    "\n",
    "    feature_names = X_t.columns.tolist()\n",
    "\n",
    "    # Logistic model\n",
    "    log_reg = LogisticRegression(\n",
    "        penalty=\"l1\",\n",
    "        solver=\"saga\",\n",
    "        C=0.1,\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=5000,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    log_reg_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"log_reg\", log_reg),\n",
    "    ])\n",
    "\n",
    "    # Fit\n",
    "    log_reg_pipe.fit(X_t, y_t)\n",
    "\n",
    "    # Predict using a custom threshold\n",
    "    # Get probability for the positive class\n",
    "    pos_class = log_reg_pipe.named_steps[\"log_reg\"].classes_[1]\n",
    "    prob_pos = log_reg_pipe.predict_proba(X_v)[:, list(log_reg_pipe.named_steps[\"log_reg\"].classes_).index(pos_class)]\n",
    "    y_pred_v = (prob_pos >= threshold).astype(int)\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_v, y_pred_v)\n",
    "    precision = precision_score(y_v, y_pred_v, average=\"macro\", zero_division=0)\n",
    "\n",
    "    # Feature importance\n",
    "    lr = log_reg_pipe.named_steps[\"log_reg\"]\n",
    "    coefs = lr.coef_\n",
    "    max_abs_coef = np.max(np.abs(coefs), axis=0)\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"max_abs_coef\": max_abs_coef,\n",
    "    }).sort_values(\"max_abs_coef\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(accuracy, precision, '\\n \\n', coef_df)\n",
    "    # return {'accuracy' : accuracy, 'precision' : precision, 'coef_df' : coef_df}\n",
    "\n",
    "def train_hist_grad(df_t, df_v, target_col='dir', threshold=None, conf_cut=0.6):\n",
    "    X_t = df_t.drop(columns=[target_col])\n",
    "    y_t = df_t[target_col]\n",
    "\n",
    "    X_v = df_v.drop(columns=[target_col])\n",
    "    y_v = df_v[target_col]\n",
    "\n",
    "    clf = HistGradientBoostingClassifier(\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        max_iter=200,\n",
    "    )\n",
    "    clf.fit(X_t, y_t)\n",
    "\n",
    "    classes_ = clf.classes_\n",
    "    probs = clf.predict_proba(X_v)\n",
    "\n",
    "    if threshold is None:\n",
    "        # standard argmax prediction\n",
    "        y_pred_v = clf.predict(X_v)\n",
    "    else:\n",
    "        if len(classes_) != 2:\n",
    "            raise ValueError(\"Thresholding only supported for binary classification.\")\n",
    "        # assume +1 is the \"positive\" class\n",
    "        idx_pos = list(classes_).index(1)\n",
    "        prob_pos = probs[:, idx_pos]\n",
    "        # map to {-1, +1}\n",
    "        y_pred_v = np.where(prob_pos >= threshold, 1, -1)\n",
    "\n",
    "    # Metrics on full val set\n",
    "    accuracy = accuracy_score(y_v, y_pred_v)\n",
    "    precision = precision_score(y_v, y_pred_v, average=\"macro\", zero_division=0)\n",
    "    print(\"Val accuracy:\", accuracy, \"Val macro precision:\", precision)\n",
    "\n",
    "    # Confidence diagnostics using argmax labels\n",
    "    y_pred_raw = classes_[probs.argmax(axis=1)]\n",
    "    max_prob = probs.max(axis=1)\n",
    "\n",
    "    mask_conf = max_prob >= conf_cut\n",
    "    y_pred_conf = y_pred_raw[mask_conf]\n",
    "    y_true_conf = y_v.to_numpy()[mask_conf]\n",
    "\n",
    "    if mask_conf.any():\n",
    "        acc_conf = accuracy_score(y_true_conf, y_pred_conf)\n",
    "    else:\n",
    "        acc_conf = np.nan\n",
    "\n",
    "    print(\"Frac of samples kept (conf >= {:.2f}):\".format(conf_cut), mask_conf.mean())\n",
    "    print(\"Acc on confident subset:\", acc_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "c4a5dc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.665790417015175 0.7160042430203591 \n",
      " \n",
      "       feature  max_abs_coef\n",
      "0      spread      1.048116\n",
      "1       vol_l      0.595716\n",
      "2  lam_comp_0      0.367429\n",
      "3  lam_comp_1      0.358462\n",
      "4       lam_2      0.283953\n",
      "5   liq_ratio      0.031960\n",
      "Val accuracy: 0.08956864619428845 Val macro precision: 0.27901436760217374\n",
      "Frac of samples kept (conf >= 0.60): 0.8022384195642649\n",
      "Acc on confident subset: 0.7496450168306705\n"
     ]
    }
   ],
   "source": [
    "train_log_reg(df_t = training_data_vol, df_v = validation_data_vol, target_col='dir', threshold=.85)\n",
    "train_hist_grad(df_t = training_data_vol, df_v = validation_data_vol, target_col='dir', threshold =.85, conf_cut=.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "d8bae72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 0.04728561755685665 Val macro precision: 0.29754385964912283\n",
      "Frac of samples kept (conf >= 0.60): 0.7776397020751185\n",
      "Acc on confident subset: 0.7593857878939982\n"
     ]
    }
   ],
   "source": [
    "train_hist_grad(df_t = training_data_vol.loc[:, cols_other_vol + ['dir']], df_v = validation_data_vol.loc[:, cols_other_vol+ ['dir']], target_col='dir', threshold =.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "98077f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 0.00036643167244194844 Val macro precision: 0.17424242424242423\n",
      "Frac of samples kept (conf >= 0.60): 0.6480423786195085\n",
      "Acc on confident subset: 0.6832161471137772\n"
     ]
    }
   ],
   "source": [
    "train_hist_grad(df_t = training_data_vol.loc[:, cols_hawks + ['dir']], df_v = validation_data_vol.loc[:, cols_hawks+ ['dir']], target_col='dir', threshold =.85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "663f3a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy acc: 0.5019483496332519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_t = training_data_dir.drop(columns=['dir'])\n",
    "y_t = training_data_dir['dir']\n",
    "\n",
    "X_v = validation_data_dir.drop(columns=['dir'])\n",
    "y_v = validation_data_dir['dir']\n",
    "\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "dummy.fit(X_t, y_t)\n",
    "y_dummy = dummy.predict(X_v)\n",
    "\n",
    "print(\"Dummy acc:\", accuracy_score(y_v, y_dummy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "1d719c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31928102078239606 0.19778724411312273 \n",
      " \n",
      "                   feature  max_abs_coef\n",
      "0              momentum_s      0.576066\n",
      "1               liq_ratio      0.145897\n",
      "2                      QI      0.142672\n",
      "3                 OFI_inc      0.098362\n",
      "4            OFI_cum_long      0.087587\n",
      "5              lam_comp_1      0.062747\n",
      "6  price_mov_derivative_l      0.047969\n",
      "7              lam_comp_0      0.027307\n",
      "8                  spread      0.007942\n",
      "9                   lam_2      0.007419\n",
      "Val accuracy: 0.6250764058679706 Val macro precision: 0.6253404622581937\n",
      "Frac of samples kept (conf >= 0.60): 0.41709963325183375\n",
      "Acc on confident subset: 0.751511265799597\n"
     ]
    }
   ],
   "source": [
    "train_log_reg(df_t = training_data_dir, df_v = validation_data_dir, target_col='dir', threshold=.5)\n",
    "train_hist_grad(df_t = training_data_dir, df_v = validation_data_dir, target_col='dir', threshold=.5, conf_cut=.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "86d9e220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dir\n",
       " 0    73183\n",
       " 1    26278\n",
       "-1    26074\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data_temp['dir'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a7030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hawkes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
